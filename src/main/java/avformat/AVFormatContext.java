package avformat;
import avcodec.AVPacket;
import com.ochafik.lang.jnaerator.runtime.Structure;
import com.sun.jna.Pointer;
import com.sun.jna.ptr.PointerByReference;
/**
 * <i>native declaration : src/main/headers/libavformat/avformat.h</i><br>
 * This file was autogenerated by <a href="http://jnaerator.googlecode.com/">JNAerator</a>,<br>
 * a tool written by <a href="http://ochafik.free.fr/">Olivier Chafik</a> that <a href="http://code.google.com/p/jnaerator/wiki/CreditsAndLicense">uses a few opensource projects.</a>.<br>
 * For help, please visit <a href="http://nativelibs4java.googlecode.com/">NativeLibs4Java</a> , <a href="http://rococoa.dev.java.net/">Rococoa</a>, or <a href="http://jna.dev.java.net/">JNA</a>.
 */
public class AVFormatContext extends Structure<AVFormatContext, AVFormatContext.ByValue, AVFormatContext.ByReference > {
	/**
	 * < Set by avformat_alloc_context.<br>
	 * C type : const AVClass*
	 */
	public Pointer av_class;
	/**
	 * Can only be iformat or oformat, not both at the same time.<br>
	 * C type : AVInputFormat*
	 */
	public avformat.AVInputFormat.ByReference iformat;
	/// C type : AVOutputFormat*
	public avformat.AVOutputFormat.ByReference oformat;
	/// C type : void*
	public Pointer priv_data;
	/// C type : ByteIOContext*
	public avformat.ByteIOContext.ByReference pb;
	public int nb_streams;
	/// C type : AVStream*[20]
	public Pointer[] streams = new Pointer[20];
	/**
	 * < input or output filename<br>
	 * C type : char[1024]
	 */
	public byte[] filename = new byte[(1024)];
	/// stream info
	public long timestamp;
	/// C type : char[512]
	public byte[] title = new byte[(512)];
	/// C type : char[512]
	public byte[] author = new byte[(512)];
	/// C type : char[512]
	public byte[] copyright = new byte[(512)];
	/// C type : char[512]
	public byte[] comment = new byte[(512)];
	/// C type : char[512]
	public byte[] album = new byte[(512)];
	/// < ID3 year, 0 if none
	public int year;
	/// < track number, 0 if none
	public int track;
	/**
	 * < ID3 genre<br>
	 * C type : char[32]
	 */
	public byte[] genre = new byte[(32)];
	/// < Format-specific flags, see AVFMTCTX_xx
	public int ctx_flags;
	/**
	 * This buffer is only needed when packets were already buffered but<br>
	 * not decoded, for example to get the codec parameters in MPEG<br>
	 * streams.<br>
	 * C type : AVPacketList*
	 */
	public avformat.AVPacketList.ByReference packet_buffer;
	/**
	 * Decoding: position of the first frame of the component, in<br>
	 * AV_TIME_BASE fractional seconds. NEVER set this value directly:<br>
	 * It is deduced from the AVStream values.
	 */
	public long start_time;
	/**
	 * Decoding: duration of the stream, in AV_TIME_BASE fractional<br>
	 * seconds. Only set this value if you know none of the individual stream<br>
	 * durations and also dont set any of them. This is deduced from the<br>
	 * AVStream values if not set.
	 */
	public long duration;
	/// decoding: total file size, 0 if unknown
	public long file_size;
	/**
	 * Decoding: total stream bitrate in bit/s, 0 if not<br>
	 * available. Never set it directly if the file_size and the<br>
	 * duration are known as FFmpeg can compute it automatically.
	 */
	public int bit_rate;
	/**
	 * av_read_frame() support<br>
	 * C type : AVStream*
	 */
	public avformat.AVStream.ByReference cur_st;
	/// C type : const uint8_t*
	public Pointer cur_ptr_deprecated;
	public int cur_len_deprecated;
	/// C type : AVPacket
	public AVPacket cur_pkt_deprecated;
	/**
	 * av_seek_frame() support<br>
	 * offset of the first packet
	 */
	public long data_offset;
	public int index_built;
	public int mux_rate;
	public int packet_size;
	public int preload;
	public int max_delay;
	/// number of times to loop output in formats that support it
	public int loop_output;
	public int flags;
	public int loop_input;
	/// decoding: size of data to probe; encoding: unused.
	public int probesize;
	/**
	 * Maximum time (in AV_TIME_BASE units) during which the input should<br>
	 * be analyzed in av_find_stream_info().
	 */
	public int max_analyze_duration;
	/// C type : const uint8_t*
	public Pointer key;
	public int keylen;
	public int nb_programs;
	/// C type : AVProgram**
	public PointerByReference programs;
	/**
	 * Forced video codec_id.<br>
	 * Demuxing: Set by user.<br>
	 * @see avcodec.AvcodecLibrary#CodecID<br>
	 * C type : CodecID
	 */
	public int video_codec_id;
	/**
	 * Forced audio codec_id.<br>
	 * Demuxing: Set by user.<br>
	 * @see avcodec.AvcodecLibrary#CodecID<br>
	 * C type : CodecID
	 */
	public int audio_codec_id;
	/**
	 * Forced subtitle codec_id.<br>
	 * Demuxing: Set by user.<br>
	 * @see avcodec.AvcodecLibrary#CodecID<br>
	 * C type : CodecID
	 */
	public int subtitle_codec_id;
	/**
	 * Maximum amount of memory in bytes to use for the index of each stream.<br>
	 * If the index exceeds this size, entries will be discarded as<br>
	 * needed to maintain a smaller size. This can lead to slower or less<br>
	 * accurate seeking (depends on demuxer).<br>
	 * Demuxers for which a full in-memory index is mandatory will ignore<br>
	 * this.<br>
	 * muxing  : unused<br>
	 * demuxing: set by user
	 */
	public int max_index_size;
	/**
	 * Maximum amount of memory in bytes to use for buffering frames<br>
	 * obtained from realtime capture devices.
	 */
	public int max_picture_buffer;
	public int nb_chapters;
	/// C type : AVChapter**
	public PointerByReference chapters;
	/// Flags to enable debugging.
	public int debug;
	/**
	 * Raw packets from the demuxer, prior to parsing and decoding.<br>
	 * This buffer is used for buffering packets until the codec can<br>
	 * be identified, as parsing cannot be done without knowing the<br>
	 * codec.<br>
	 * C type : AVPacketList*
	 */
	public avformat.AVPacketList.ByReference raw_packet_buffer;
	/// C type : AVPacketList*
	public avformat.AVPacketList.ByReference raw_packet_buffer_end;
	/// C type : AVPacketList*
	public avformat.AVPacketList.ByReference packet_buffer_end;
	/// C type : AVMetadata*
	public avformat.AVMetadata.ByReference metadata;
	public int raw_packet_buffer_remaining_size;
	/**
	 * Start time of the stream in real world time, in microseconds<br>
	 * since the unix epoch (00:00 1st January 1970). That is, pts=0<br>
	 * in the stream was captured at this real world time.<br>
	 * - encoding: Set by user.<br>
	 * - decoding: Unused.
	 */
	public long start_time_realtime;
	public AVFormatContext() {
		super();
		initFieldOrder();
	}
	protected void initFieldOrder() {
		setFieldOrder(new java.lang.String[]{"av_class", "iformat", "oformat", "priv_data", "pb", "nb_streams", "streams", "filename", "timestamp", "title", "author", "copyright", "comment", "album", "year", "track", "genre", "ctx_flags", "packet_buffer", "start_time", "duration", "file_size", "bit_rate", "cur_st", "cur_ptr_deprecated", "cur_len_deprecated", "cur_pkt_deprecated", "data_offset", "index_built", "mux_rate", "packet_size", "preload", "max_delay", "loop_output", "flags", "loop_input", "probesize", "max_analyze_duration", "key", "keylen", "nb_programs", "programs", "video_codec_id", "audio_codec_id", "subtitle_codec_id", "max_index_size", "max_picture_buffer", "nb_chapters", "chapters", "debug", "raw_packet_buffer", "raw_packet_buffer_end", "packet_buffer_end", "metadata", "raw_packet_buffer_remaining_size", "start_time_realtime"});
	}
	protected ByReference newByReference() { return new ByReference(); }
	protected ByValue newByValue() { return new ByValue(); }
	protected AVFormatContext newInstance() { return new AVFormatContext(); }
	public static AVFormatContext[] newArray(int arrayLength) {
		return Structure.newArray(AVFormatContext.class, arrayLength);
	}
	public static class ByReference extends AVFormatContext implements Structure.ByReference {
		
	};
	public static class ByValue extends AVFormatContext implements Structure.ByValue {
		
	};
	
}
